---
layout:
title: Charles London, University of Oxford
---

<!DOCTYPE html>
<html lang="en">
<head>
<title>Charles London, University of Oxford</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="/assets/styles.css">
</head>

<body>

<!-- Header -->
<div class="row">
  <div class="column left">
    <h1>Charles London</h1>
    <h3>DPhil Student</h3>
    <p><a href = "https://www.cs.ox.ac.uk/">Department of Computer Science</a><br>University of Oxford<br>Wolfson Building, Parks Road, Oxford, OX1 3QD</p>
    <p>E-mail: <i>charles.london"at"cs.ox.ac.uk</i></p>
    <p><a href = "/assets/files/London_CV.pdf", target="_blank">CV</a> &nbsp <a href="blog.html">Blog and notes</a></p>
  </div>
  <div class="column right">
  	<br><br>
  	<img src="assets/images/web_photo_1.jpeg" class="profile-photo" style="width: 200px">
  </div>
</div>

<div class="menu">
  Jump to: &nbsp <a href = "#about">About me</a> &nbsp <a href = "#collabs">Ideas and collaborations</a> &nbsp <a href = "#research">Research</a> &nbsp <a href = "#teaching">Teaching</a> &nbsp <a href = "#other">Miscellaneous</a>
</div>


<hr>
<div class="section" id="about">
	<h3>About me</h3>
	I am a DPhil student in computer science at the University of Oxford, supervised by <a href="https://www.cs.ox.ac.uk/people/varun.kanade/website/">Prof. Varun Kanade</a>. I work mainly in machine learning theory, particularly theory of LLMs, and continual or open-ended learning. I am also interested in machine learning for mathematical reasoning.

    Before starting my DPhil I spent two years at <a href="https://www.quantinuum.com/">Quantinuum</a>, working on quantum machine learning with <a href="https://www.cs.ox.ac.uk/people/bob.coecke/">Bob Coecke</a>, <a href="https://scholar.google.co.uk/citations?user=bBnvK8cAAAAJ&hl=en">Stephen Clark</a> and <a href="https://sites.google.com/site/dimkart/">Dimitri Kartsaklis</a>. Prior to this I completed my MSc degree in computer science at Oxford, graduating in early 2022, with my dissertation on generalization bounds supervised by <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/">Prof. Yarin Gal</a>. I completed my BA in computer science at the University of Cambridge, graduating in 2019. My undergraduate dissertation was supervised by <a href="https://www.cl.cam.ac.uk/~pl219/">Prof. Pietro Li√≤</a> and focused on semi-supervised machine learning methods for cancer classification.

</div>

<hr>

<div class="section" id="collabs">
    <h3>Ideas and collaborations</h3>
    I am always interested in discussing ideas and potential collaborations. I have more ideas than I have time, and a few of my current ideas are:
    <ul class="b">
        <li>Mine GitHub for Lean proof patches, to train an LLM to construct proofs as in <a href="https://arxiv.org/abs/2502.18449">SWE-RL.</a></li>
        <li>A library for automated cross-layer transcoder construction (see <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">this from Anthropic</a>) in the same vein as <a href="https://github.com/EleutherAI/sparsify">Eleuther's sparsify.</a></li>
        <li>Using cross-layer transcoders to analyse how different parts of the LLM training pipeline affect medium- and long-term planning (see <a href="https://github.com/EleutherAI/sparsify">here</a>).</li>
        <li>Anything to do with LLM theory, especially from a complexity theory perspective. See, for example, <a href="https://arxiv.org/abs/2402.12875">this paper</a> on CoT and complexity classes, <a href="https://arxiv.org/abs/2412.02975">this paper</a> on complexity-theoretic properties of multi-layer LLMs, or <a href="https://arxiv.org/abs/2503.14337">this paper</a> on optimal space complexity for LLMs.</li>
        <li>Anything to do with continual learning or continual RL theory, or applications to LLM agents. I'm particularly interested in the case of long time-horizons, where we see many tasks, some repeated, compute has some cost, and we want to minimize regret. I am also particularly interested in infinite memory, limited compute continual learning.</li>
    </ul>
    <br>
    If any of these ideas sound interesting to you, or if you have your own ideas that you would like to discuss, please get in touch at the email address above. I'm also just happy to chat!
</div>

<hr>

<div class="section" id="research">
	<h3>Research &nbsp; <a href = "https://scholar.google.com/citations?user=ghU-4hUAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-2x"></i></a> &nbsp; <a href = "https://dblp.org/pid/361/6774.html"><i class="ai ai-dblp-square ai-2x"></i></a></h3>
    <b>Research interests:</b><br>
    Learning theory, LLM theory, continual learning, "agents", online learning, meta-learning, deep learning, generalization.<br>
    <br>
</div>

<div class="section compact">
    <b>Selected publications:</b><BR>
    <p style="text-indent: 10px;">
    <ul class="b">
        <li>Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers (2025).<br>
            <i>NeurIPS</i> <a href="https://arxiv.org/abs/2505.21024">[arXiv]</a>
        <li>REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites (2025).<br>
            <i>NeurIPS Datasets and Benchmarks</i> <a href="https://arxiv.org/abs/2504.11543">[arXiv]</a>
        <li>Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (2025).<br>
            <i>ICML Position Paper</i> <a href="https://arxiv.org/abs/2502.21009">[arXiv]</a>
        <li>Disentangling Feature Learning from Generalization in Neural Networks (2025).<br>
            <i>ICML Workshop on High-Dimensional Learning Dynamics</i> <a href="https://arxiv.org/abs/2507.19680">[arXiv]</a>
        <li>Peptide Binding Classification on Quantum Computers (2024).<br>
            <i>Springer Quantum Machine Intelligence</i> <a href="https://link.springer.com/article/10.1007/s42484-024-00154-3">[QMI]</a>  <a href="https://arxiv.org/abs/2311.15696">[arXiv]</a>
    </ul>
    </p>
</div>

<div class="section compact">
    <b>Preprints and submitted papers:</b><BR>
    <p style="text-indent: 10px;">
    <ul class="b">
        <li>Characterising the Inductive Biases of Neural Networks on Boolean Data (2025).<br>
            <a href="https://arxiv.org/abs/2505.24060">[arXiv]</a>
        <li>Exploiting the Equivalence between Quantum Neural Networks and Perceptrons (2024).<br>
            <a href="https://arxiv.org/abs/2407.04371">[arXiv]</a>
    </ul>
    </p>
</div>

<hr>

<div class="section" id="teaching">
<h3>Teaching</h3>
    <p style="text-indent: 10px;">
    Departmental Tutor - <a href = "https://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-MT2023/index.html">Computational Learning Theory</a> (Michaelmas 2023, 2024)
    </p>
</div>

<hr>

<div class="section" id="other">
	<h3>Miscellaneous</h3>

    <p>
	<b>Other interests:</b><br>
    Economics of AI, theoretical CS, statistical physics, optimization, game theory, Arsenal football club, American football, spy novels, sci-fi.
    </p>

    <p><b>Favourite fiction books (no particular order):</b>
        Gorky Park, Blood Meridian, Brave New World, Leave it to Psmith, Do Androids Dream of Electric Sheep?, Moving Pictures, Berlin Game, All the Pretty Horses, Necropolis, Foundation, Catch-22, Neuromancer
    </p>

    <p>
    <b>Favourite films (no particular order):</b><br>
    In Bruges, No Country for Old Men, The Handmaiden, Shrek 2, Blade Runner, The Nice Guys
    </p>

    <p><b>Favourite video games (no particular order):</b><br>
    Dragon Age: Origins, Super Mario Galaxy, Fallout: New Vegas, Hollow Knight, Baldur's Gate 3, Assassin's Creed: Black Flag, Metal Gear Solid V, Outer Wilds
    </p>
</div>

<footer>
    <p>This website was adapted from <a href="https://utstat.toronto.edu/leonard/">Ting-Kam Leonard Wong</a>. Last updated {{ site.time | date: "%d/%m/%Y" }}.</p>
</footer>

</body>

<script data-goatcounter="https://le-big-mac.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

</html>